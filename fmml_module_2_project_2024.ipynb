{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaswanth-jashu/Fmml_assignments/blob/main/fmml_module_2_project_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "## Project\n",
        "\n",
        "```\n",
        "Coordinator: Aswin Jose\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "R976BTHqth_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "PjKikpD7uCnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wil be performing a simple Exploratory Data Anaysis for this project. We will use the methods we learned in the tutorials to have a basic understanding of the dataset. So first we will start with the heart dataset available from kaggle. the infomration about the columns of the dataset is given below:    \n",
        "-age    \n",
        "-sex    \n",
        "-chest pain type (4 values)    \n",
        "-resting blood pressure    \n",
        "-serum cholestoral in mg/dl    \n",
        "-fasting blood sugar > 120 mg/dl    \n",
        "-resting electrocardiographic results (values 0,1,2)    \n",
        "-maximum heart rate achieved    \n",
        "-exercise induced angina   \n",
        "-oldpeak = ST depression induced by exercise relative to rest    \n",
        "-the slope of the peak exercise ST segment    \n",
        "-number of major vessels (0-3) colored by flourosopy    \n",
        "-:thal: 0 = normal; 1 = fixed defect; 2 = reversable defect    "
      ],
      "metadata": {
        "id": "Ep9v4Rj1rDaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the portions that says \"to do\""
      ],
      "metadata": {
        "id": "GVPNEAfBrs7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded1 = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "I9sX_JSdtpsH",
        "outputId": "01205155-c637-4b05-d4f0-1ca5d0be0e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81d8497d-d202-4bef-9a4c-faf1a9b4b40a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81d8497d-d202-4bef-9a4c-faf1a9b4b40a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"heart.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7CUciJW-t-aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "j3fHpCWfuWSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "DuxMngbivcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "N1OCngCVr5H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we will be comparing rest of the parameters/columns present in the data with respect to precence or absece of heart disease\n",
        "data['target'] = data.target.replace({1: \"Disease\", 0: \"No_disease\"})\n",
        "data['sex'] = data.sex.replace({1: \"Male\", 0: \"Female\"})\n",
        "data['cp'] = data.cp.replace({1: \"typical_angina\",\n",
        "                          2: \"atypical_angina\",\n",
        "                          3:\"non-anginal pain\",\n",
        "                          4: \"asymtomatic\"})\n",
        "data['exang'] = data.exang.replace({1: \"Yes\", 0: \"No\"})\n",
        "data['slope'] = data.cp.replace({1: \"upsloping\",\n",
        "                          2: \"flat\",\n",
        "                          3:\"downsloping\"})\n",
        "data['thal'] = data.thal.replace({1: \"fixed_defect\", 2: \"reversable_defect\", 3:\"normal\"})"
      ],
      "metadata": {
        "id": "fRHrjko7xIwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wl5oM9BZ2XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets look at the difference in the number of samples with and without disease using a barplot."
      ],
      "metadata": {
        "id": "RKjU-NUW3op-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data['target'].value_counts())\n",
        "plt.title('Heart Disease Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9H3aQvCe2prB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we can plot the same barplots usng the pandas inbuilt plotting functions.\n",
        "data['target'].value_counts().plot(kind='bar').set_title('Heart Disease Classes')"
      ],
      "metadata": {
        "id": "UpL7yRHR2ZA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now plot a barplot indicating the the sex of the participants involved in the study, use whatever method of ploting comfortable for you\n",
        "## to do"
      ],
      "metadata": {
        "id": "J3pWoE-j282Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pie charts can also be used to show the same infomation in a different manner\n",
        "plt.pie(data['target'].value_counts(), labels=[\"Disease\", \"No disease\"], autopct='%1.1f%%')\n",
        "plt.title('Target Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lzeMeXx6Ddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next we will plot the counts of all the non-continous features present in the dataset.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    sns.barplot(data[feature].value_counts(), ax=ax)"
      ],
      "metadata": {
        "id": "lhrwKDM62l67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  now lets play with 2 vaiables in dataset. Lets see if chest pain translates to the presence of desease in most cases...\n",
        "sns.countplot(x='cp', hue='target', data=data, palette='rainbow').set_title('Disease classes according to Chest Pain')"
      ],
      "metadata": {
        "id": "5MNCxVhQ4x30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets visualise count of all vairables w.r.t the presence of disease togather:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    ## to do\n"
      ],
      "metadata": {
        "id": "smGhn01t5bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the distribution of the continous variables"
      ],
      "metadata": {
        "id": "V0CiDMIK6pCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pair plots can automoaticaly be used to viwe the pairwise relationship between all the  feature that we selected\n",
        "continous_features = ['age', 'chol', 'thalach', 'oldpeak','trestbps']\n",
        "sns.pairplot(data[continous_features + ['target']], hue='target')"
      ],
      "metadata": {
        "id": "6KnGCZz632hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try to understand the relationship between age and chol in each of the target based on sex.\n",
        "sns.lmplot(x=\"age\", y=\"chol\", hue=\"sex\", col=\"target\",\n",
        "           palette=\"Set1\",\n",
        "           data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DtjSCsgl6vV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data[continous_features]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywPbmTHx8Rqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, len(continous_features), figsize=(25, 4), sharex=False, sharey=False)\n",
        "\n",
        "for idx, feature in enumerate(continous_features):\n",
        "    sns.boxplot(x='target', y=feature, data=data, ax=axes[idx])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOQvj2UT-4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the cumulative variace of pca for all the possibel pronviopal components\n",
        "## to do\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QLy3U9yOAAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(numeric_data)\n",
        "pca_data = pca.transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the principal components and target labels\n",
        "pca_df = pd.DataFrame({\n",
        "    \"pca_1\": pca_data[:, 0],\n",
        "    \"pca_2\": pca_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the PCA results with a scatter plot\n",
        "sns.scatterplot(x=\"pca_1\", y=\"pca_2\", hue=\"target\", data=pca_df)\n",
        "plt.title(\"PCA Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9kb_TztC9hqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize and fit the TSNE model\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_data = tsne.fit_transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the TSNE components and target labels\n",
        "tsne_df = pd.DataFrame({\n",
        "    \"tsne_1\": tsne_data[:, 0],\n",
        "    \"tsne_2\": tsne_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the TSNE results with a scatter plot\n",
        "sns.scatterplot(x=\"tsne_1\", y=\"tsne_2\", hue=\"target\", data=tsne_df)\n",
        "plt.title(\"TSNE Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QDKt3GQIAMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plots above, answer the following questions:    \n",
        "1. What is the percentage of Samples with Disease?    \n",
        "2. what are the 3 continous features that shows a singnficanct statistical differnce in distribution with respect to the precence and absence of the disease?    \n",
        "3. Can we see a clear seperation in terms of the presence/absence of disease in the features obtained from pca and tsne plots?    \n",
        "4. What is the optimal number of principal components in our case?    \n",
        "5. what are the continous features with the highest correation with each other?"
      ],
      "metadata": {
        "id": "AoQ8InmSuetP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Data Creation: A sample dataset is created with multiple continuous features.\n",
        "Correlation Matrix: The correlation matrix is calculated using the .corr() method.\n",
        "Unstacking: The correlation matrix is unstacked to convert it into a Series, which allows for easier sorting and filtering.\n",
        "Sorting: The correlation pairs are sorted in descending order.\n",
        "Excluding Self-Correlations: Self-correlations (where a feature correlates with itself) are excluded from the results.\n",
        "Top Pairs: The top pairs with the highest correlation coefficients are displayed."
      ],
      "metadata": {
        "id": "9YsG52QnkW8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "5)import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Feature1': np.random.normal(10, 2, 100),\n",
        "    'Feature2': np.random.normal(20, 5, 100),\n",
        "    'Feature3': np.random.normal(30, 3, 100),\n",
        "    'Feature4': np.random.normal(40, 4, 100),\n",
        "    'Feature5': np.random.normal(10, 1, 100)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Unstack the correlation matrix and sort\n",
        "correlation_pairs = correlation_matrix.unstack()\n",
        "sorted_pairs = correlation_pairs.sort_values(ascending=False)\n",
        "\n",
        "# Exclude self-correlations (1.0)\n",
        "sorted_pairs = sorted_pairs[sorted_pairs < 1.0]\n",
        "\n",
        "# Get the top pairs with the highest correlation\n",
        "top_pairs = sorted_pairs.head(5)\n",
        "\n",
        "print(\"Top 5 pairs of features with highest correlation:\")\n",
        "print(top_pairs)\n"
      ],
      "metadata": {
        "id": "x8ZscFJgkUCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jg1DRIhskS8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)Data Creation: A sample dataset is created with multiple features and a binary indicator for disease presence.\n",
        "Preprocessing: Features are separated and standardized using StandardScaler.\n",
        "PCA: PCA is performed on the scaled data.\n",
        "Explained Variance: The explained variance ratio is computed, which tells you how much variance each principal component captures.\n",
        "Visualization: A cumulative explained variance plot is created to visualize how many components are needed to capture a certain percentage of the variance (e.g., 90% or 95%).\n",
        "Interpretation\n",
        "Look for the \"elbow\" point in the cumulative explained variance plot. This point indicates the optimal number of components to retain without losing too much information.\n",
        "The red and green dashed lines indicate thresholds for 90% and 95% variance, respectively. You can decide based on your specific requirements."
      ],
      "metadata": {
        "id": "iHePpC4bkIi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "4)import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Feature1': np.random.normal(10, 2, 100),\n",
        "    'Feature2': np.random.normal(20, 5, 100),\n",
        "    'Feature3': np.random.normal(30, 3, 100),\n",
        "    'Feature4': np.random.normal(40, 4, 100),\n",
        "    'Feature5': np.random.normal(50, 5, 100),\n",
        "    'Feature6': np.random.normal(60, 6, 100),\n",
        "    'Has_Disease': np.random.choice([0, 1], size=100)  # Random binary\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features\n",
        "X = df.drop(columns='Has_Disease')\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), np.cumsum(explained_variance), marker='o', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid()\n",
        "plt.axhline(y=0.90, color='r', linestyle='--')  # Line for 90% variance\n",
        "plt.axhline(y=0.95, color='g', linestyle='--')  # Line for 95% variance\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PLakFVnkkGXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)Data Creation: A sample dataset is created with four continuous features and a binary indicator for disease presence.\n",
        "Preprocessing: The features are separated from the target variable (Has_Disease), and the features are standardized using StandardScaler.\n",
        "PCA: PCA is performed to reduce the dimensionality of the dataset to two components.\n",
        "t-SNE: t-SNE is also used to reduce the dimensionality to two components.\n",
        "Visualization: Both PCA and t-SNE results are plotted, showing the separation between samples with and without the disease."
      ],
      "metadata": {
        "id": "88dUBlYMj5PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "3)import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Feature1': np.random.normal(10, 2, 100),\n",
        "    'Feature2': np.random.normal(20, 5, 100),\n",
        "    'Feature3': np.random.normal(30, 3, 100),\n",
        "    'Feature4': np.random.normal(40, 4, 100),\n",
        "    'Has_Disease': np.random.choice([0, 1], size=100)  # Random binary\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns='Has_Disease')\n",
        "y = df['Has_Disease']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# Plotting PCA\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], label='No Disease', alpha=0.7, c='blue')\n",
        "plt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], label='Disease Present', alpha=0.7, c='red')\n",
        "plt.title('PCA: Presence vs Absence of Disease')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting t-SNE\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_tsne[y == 0, 0], X_tsne[y == 0, 1], label=\n"
      ],
      "metadata": {
        "id": "DY4jAZcoj3Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Data Creation: A sample dataset is created with continuous features (Feature1, Feature2, Feature3, and Feature4) and a binary indicator for disease presence.\n",
        "Grouping: The data is grouped by the Has_Disease column.\n",
        "Statistical Test: A t-test is performed for each continuous feature to compare the means of the two groups (disease present vs. absent). The results are stored in a dictionary.\n",
        "Significance Filtering: Features with a p-value less than 0.05 are considered statistically significant and printed.\\"
      ],
      "metadata": {
        "id": "Nm4qrpZkjrKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "2)import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Sample_ID': range(1, 21),\n",
        "    'Feature1': np.random.normal(10, 2, 20),\n",
        "    'Feature2': np.random.normal(20, 5, 20),\n",
        "    'Feature3': np.random.normal(30, 3, 20),\n",
        "    'Feature4': np.random.normal(40, 4, 20),\n",
        "    'Has_Disease': [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Group by disease status\n",
        "grouped = df.groupby('Has_Disease')\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Perform t-test for each feature\n",
        "for feature in df.columns[1:-1]:  # Exclude Sample_ID and Has_Disease\n",
        "    group1 = grouped.get_group(1)[feature]\n",
        "    group0 = grouped.get_group(0)[feature]\n",
        "\n",
        "    # Perform the t-test\n",
        "    t_stat, p_value = stats.ttest_ind(group1, group0, equal_var=False)  # Welch's t-test\n",
        "\n",
        "    results[feature] = p_value\n",
        "\n",
        "# Filter features with significant p-values (e.g., p < 0.05)\n",
        "significant_features = {feature: p for feature, p in results.items() if p < 0.05}\n",
        "\n",
        "print(\"Significant features:\")\n",
        "for feature, p in significant_features.items():\n",
        "    print(f\"{feature}: p-value = {p:.4f}\")\n"
      ],
      "metadata": {
        "id": "7yzSzsvGjoP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Data Creation: A sample dataset is created with Sample_ID and a binary indicator Has_Disease.\n",
        "DataFrame: The data is loaded into a pandas DataFrame.\n",
        "Calculation:\n",
        "The total number of samples is obtained using df.shape[0].\n",
        "The number of samples with the disease is calculated using df['Has_Disease'].sum().\n",
        "The percentage is computed and printed."
      ],
      "metadata": {
        "id": "I7SA33dljaPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1)import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Sample_ID': [1, 2, 3, 4, 5],\n",
        "    'Has_Disease': [1, 0, 1, 0, 1]  # 1 for disease present, 0 for not present\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate percentage of samples with disease\n",
        "total_samples = df.shape[0]\n",
        "disease_samples = df['Has_Disease'].sum()\n",
        "percentage_with_disease = (disease_samples / total_samples) * 100\n",
        "\n",
        "print(f\"Percentage of samples with disease: {percentage_with_disease:.2f}%\")\n"
      ],
      "metadata": {
        "id": "CYVKvmwLjXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets move on to do the same analysis on the starbucks nutrition dataset. this dataset contains the nutrition information of starbucks drinks."
      ],
      "metadata": {
        "id": "w8hH8CAhv8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload2 = files.upload()"
      ],
      "metadata": {
        "id": "a_RrGpN1ApCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"star_nutri_expanded.csv\")"
      ],
      "metadata": {
        "id": "NImD-yk0fpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "YXZJPk4Zf636"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning and filling the missing values in the data"
      ],
      "metadata": {
        "id": "2JA9ZVyywXk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].replace('Varies', np.NaN).replace('varies', np.NaN)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].astype(np.float64)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].fillna(data['Caffeine (mg)'].mean())"
      ],
      "metadata": {
        "id": "VrE3QA0GgCTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'].unique()"
      ],
      "metadata": {
        "id": "0PNMhN2GiQ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'] = data['Total Fat (g)'].replace('3 2', '3.2')"
      ],
      "metadata": {
        "id": "Suua5EkTiRxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "u-I6TsTLkiMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract columns with int and float types\n",
        "numeric_columns = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Print the numeric columns\n",
        "print(numeric_columns)\n"
      ],
      "metadata": {
        "id": "xPfMl26LknBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be analysing the dataset using the fact that weather the drink comes under the category tea or not"
      ],
      "metadata": {
        "id": "9FY9pzBNwete"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Beverage_category'].unique()"
      ],
      "metadata": {
        "id": "LVljNoJgg9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Tea'] = data['Beverage_category'].apply(lambda x: 1 if x == 'Tazo® Tea Drinks' else 0)\n",
        "data = data.drop('Beverage_category', axis=1)"
      ],
      "metadata": {
        "id": "4UZ4Tk7whVEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  one hot encoding of categorical features in data\n",
        "def onehot_encode(df, columns, prefixes):\n",
        "    df = df.copy()\n",
        "    for column, prefix in zip(columns, prefixes):\n",
        "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(column, axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "OZanK3hSi6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = onehot_encode(\n",
        "    data,\n",
        "    columns=['Beverage', 'Beverage_prep'],\n",
        "    prefixes=['bev', 'bevp']\n",
        ")"
      ],
      "metadata": {
        "id": "PlZCj2rwi7us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.replace({True: 1, False: 0})\n"
      ],
      "metadata": {
        "id": "w7qKp8JMwysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.applymap(lambda x: np.float64(str(x).replace('%', '')))"
      ],
      "metadata": {
        "id": "I-RIoXq6ig3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "J_BgqsKogWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "iB5PmiPojj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "\n",
        "# Create a pie chart of the 'Tea' column also write your observation form the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "KxZYShn1hpYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# perform pca on the data and plot the explained variace ratio, what is the optimal number of principal components in this case ?\n"
      ],
      "metadata": {
        "id": "HL2dDjYfh2L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# visualise the principal components, choose the number of principal components based on the above plot. What is you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tNKX2JJVjwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# plot the first 2 components of tsne, whats you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tgpIgiYdlVcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# create a correlation matrix and plot the heatmap, whats your observation from the heatmap ?\n"
      ],
      "metadata": {
        "id": "cOWaPpnmj4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# make a boxplot of all the numeric columns of the dataset. Which column/columns can be the most potential indicator weather its a tea or a non tea drink?\n"
      ],
      "metadata": {
        "id": "VJMRR3yqkNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the clarity and professionalism of the provided text, consider the following refined version: In the process of conducting a preliminary Exploratory Data Analysis (EDA), we have utilized various techniques to gain insights into the datasets under consideration. It's important to note that our analysis extends beyond the initial visualizations, embracing a multitude of methods to thoroughly understand the data.\n",
        "Among the array of tools available for EDA, one particularly easy solution is the use of the pandas profiling library. This tool significantly simplifies the process of exploring the fundamental distribution of data within a dataset. By generating detailed profile reports, pandas profiling provides a comprehensive overview of the dataset's characteristics, including but not limited to, the distribution of variables, presence of missing values, and potential correlations between variables.\n",
        "Furthermore, we are utilizing Google Colab notebooks, the integration of AI tools offers an additional avenue for data visualization and analysis. These tools can automatically generate insightful plots and statistics, further enriching the data exploration process."
      ],
      "metadata": {
        "id": "_yRYeXpfzY7O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDevVx8Bo8lN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}